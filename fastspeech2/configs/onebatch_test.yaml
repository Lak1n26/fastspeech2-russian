defaults:
  - model: fastspeech2
  - writer: cometml
  - datasets: ruslan_overfit_split
  - dataloader: overfit
  - _self_

vocoder:
  enabled: true
  checkpoint_path: "saved/waveglow/model.ckpt-610000.pt"
  params_path: "waveglow_params.json"

name: fastspeech2_overfit_test

n_gpu: 1

loss_function:
  _target_: fastspeech2.loss.FastSpeech2LossWrapper
  mel_loss_type: mae
  lambda_mel: 1.0
  lambda_duration: 1.0
  lambda_pitch: 1.0
  lambda_energy: 1.0

metrics:
  train:
    - _target_: fastspeech2.metrics.MelSpectrogramMAE
      name: mel_mae
    - _target_: fastspeech2.metrics.MelSpectrogramMSE
      name: mel_mse
    - _target_: fastspeech2.metrics.DurationAccuracy
      name: duration_accuracy
      tolerance: 5
    - _target_: fastspeech2.metrics.DurationMAE
      name: duration_mae
    - _target_: fastspeech2.metrics.PitchMAE
      name: pitch_mae
    - _target_: fastspeech2.metrics.EnergyMAE
      name: energy_mae

  val:
    - _target_: fastspeech2.metrics.MelSpectrogramMAE
      name: mel_mae

  inference:
    - _target_: fastspeech2.metrics.MelSpectrogramMAE
      name: mel_mae

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001
  betas: [0.9, 0.98]
  eps: 1e-9
  weight_decay: 0.0

lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 1000
  gamma: 1.0

trainer:
  n_epochs: 6
  epoch_len: 500
  log_step: 250
  save_dir: saved/
  save_period: 10
  verbosity: 2

  device: auto
  device_tensors: ["text_tokens", "mel", "duration", "pitch", "energy", "emotion"]
  use_amp: true

  val_every_n_epoch: 1

  grad_norm_clip: 1.0
  accumulation_steps: 1

  monitor: "min val_loss"
  early_stop: 1000
  resume_from: null
  override: false

  seed: 42
  deterministic: true

writer:
  run_name: "onebatch_test"
